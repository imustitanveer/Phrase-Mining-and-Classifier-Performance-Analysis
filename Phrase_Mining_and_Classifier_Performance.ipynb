{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1\n",
        "a. Abnormal Scores Analysis"
      ],
      "metadata": {
        "id": "MKRlJbrrhbKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"score\", \"phrase\"])\n",
        "\n",
        "def analyze_scores(data, high_threshold=0.9, low_threshold=0.6, display_count=5):\n",
        "    # High score but potentially non-meaningful phrases\n",
        "    high_score_potential_non_phrases = data[data['score'] > high_threshold].head(display_count)\n",
        "\n",
        "    # Low score but potentially meaningful phrases\n",
        "    low_score_potential_good_phrases = data[data['score'] < low_threshold].head(display_count)\n",
        "\n",
        "    return high_score_potential_non_phrases, low_score_potential_good_phrases\n",
        "\n",
        "def plot_score_distribution(data, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(data['score'], bins=50, alpha=0.7)\n",
        "    plt.title(f'Score Distribution in {title}')\n",
        "    plt.xlabel('Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_length_vs_score(data, title):\n",
        "    # Handle non-string (like NaN or float) in 'phrase' column\n",
        "    data['length'] = data['phrase'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.scatterplot(data=data, x='score', y='length', alpha=0.5)\n",
        "    plt.title(f'Score vs Phrase Length in {title}')\n",
        "    plt.xlabel('Score')\n",
        "    plt.ylabel('Phrase Length')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def detect_outliers(data, z_thresh=3):\n",
        "    data['z_score'] = stats.zscore(data['score'])\n",
        "    outliers = data[(data['z_score'] > z_thresh) | (data['z_score'] < -z_thresh)]\n",
        "    return outliers\n",
        "\n",
        "def analyze_correlation(data, title):\n",
        "    correlation = data['score'].corr(data['length'])\n",
        "    print(f\"Correlation between score and phrase length in {title}: {correlation:.2f}\")\n",
        "\n",
        "# File paths - replace with your file paths\n",
        "auto_phrase_path = 'AutoPhrase.txt'\n",
        "single_word_path = 'AutoPhrase_single-word.txt'\n",
        "multi_word_path = 'AutoPhrase_multi-words.txt'\n",
        "\n",
        "# Load data\n",
        "auto_phrase_data = load_data(auto_phrase_path)\n",
        "single_word_data = load_data(single_word_path)\n",
        "multi_word_data = load_data(multi_word_path)\n",
        "\n",
        "# Analyzing each file separately\n",
        "print(\"Analyzing AutoPhrase.txt\")\n",
        "high, low = analyze_scores(auto_phrase_data)\n",
        "print(\"High score potential non-phrases:\\n\", high)\n",
        "print(\"\\nLow score potential good phrases:\\n\", low)\n",
        "\n",
        "print(\"\\nAnalyzing AutoPhrase_single-word.txt\")\n",
        "high, low = analyze_scores(single_word_data)\n",
        "print(\"High score potential non-phrases:\\n\", high)\n",
        "print(\"\\nLow score potential good phrases:\\n\", low)\n",
        "\n",
        "print(\"\\nAnalyzing AutoPhrase_multi-words.txt\")\n",
        "high, low = analyze_scores(multi_word_data)\n",
        "print(\"High score potential non-phrases:\\n\", high)\n",
        "print(\"\\nLow score potential good phrases:\\n\", low)\n",
        "\n",
        "# Plotting the score distributions for each file\n",
        "plot_score_distribution(auto_phrase_data, \"AutoPhrase.txt\")\n",
        "plot_score_distribution(single_word_data, \"AutoPhrase_single-word.txt\")\n",
        "plot_score_distribution(multi_word_data, \"AutoPhrase_multi-words.txt\")\n",
        "\n",
        "# Analysis for each file\n",
        "for data, title in zip([auto_phrase_data, single_word_data, multi_word_data],\n",
        "                       ['AutoPhrase', 'AutoPhrase Single Word', 'AutoPhrase Multi Words']):\n",
        "    print(f\"\\nAnalysis for {title}:\")\n",
        "    plot_length_vs_score(data, title)\n",
        "    outliers = detect_outliers(data)\n",
        "    print(f\"Outliers detected: {len(outliers)}\")\n",
        "    analyze_correlation(data, title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "3_lzHzxnhczt",
        "outputId": "e7b097aa-0f23-467f-a4d3-4687d9ff92e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'AutoPhrase.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7892a9e76a99>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mauto_phrase_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_phrase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0msingle_word_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_word_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mmulti_word_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_word_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7892a9e76a99>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"phrase\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AutoPhrase.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###a. Explanation\n"
      ],
      "metadata": {
        "id": "cVJs4KzWiAFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw4vhzN_mZgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348e0cc5-fe8f-4d92-b805-9d65b7185c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oql, c++, extending, c++\n",
            "transaction management, multidatabase systems\n",
            "overview\n",
            "multimedia, information\n",
            "active, database systems\n",
            "object-oriented, dbmss, early\n",
            "distributed, databases\n",
            "an object-oriented, dbms, war, story, developing, genome, mapping, database, c++\n",
            "cooperative, multiuser\n",
            "architecture, multidatabase\n",
            "physical object, management\n",
            "introduction, next-generation, database, technology\n",
            "object-oriented, database systems, reality\n",
            "introduction, technology, interoperating, legacy databases\n",
            "resolving, schematic, multidatabase systems\n",
            "performance benchmark, object-oriented, database systems\n",
            "object-oriented, databases\n",
            "solution, managing, e, p, data\n",
            "c++, object database\n",
            "authorization, object-oriented, databases\n"
          ]
        }
      ],
      "source": [
        "def process_line(line):\n",
        "    # Split phrases by spaces and replace underscores with spaces in each phrase\n",
        "    phrases = [' '.join(phrase.split('_')).lower() for phrase in line.split()]\n",
        "    # Join phrases with commas\n",
        "    return ', '.join(phrases)\n",
        "\n",
        "# Path to the file\n",
        "file_path = 'segmentation.txt'\n",
        "\n",
        "# Process the entire file and save the results back to the same file\n",
        "with open(file_path, 'r') as file:\n",
        "    processed_lines = [process_line(line.strip()) for line in file]\n",
        "\n",
        "# Writing back to the same file\n",
        "with open(file_path, 'w') as file:\n",
        "    for line in processed_lines:\n",
        "        file.write(line + '\\n')\n",
        "\n",
        "# Print the first 20 lines from the newly processed file\n",
        "with open(file_path, 'r') as file:\n",
        "    for _ in range(20):\n",
        "        print(next(file).strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c)"
      ],
      "metadata": {
        "id": "5_9IgKl6DTMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Read and preprocess the data\n",
        "with open('segmentation.txt', 'r') as file:\n",
        "    documents = [simple_preprocess(line) for line in file]\n",
        "\n",
        "# Create and train the Word2Vec model\n",
        "model = Word2Vec(documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Phrases of interest\n",
        "phrases = ['computer science', 'resource management', 'natural language processing',\n",
        "           'performance evaluation', 'data structure', 'artificial intelligence']\n",
        "\n",
        "# Function to find and format the ten most similar phrases\n",
        "def find_similar_phrases(phrase):\n",
        "    phrase_vector = sum(model.wv[word] for word in simple_preprocess(phrase)) / len(simple_preprocess(phrase))\n",
        "    similar_phrases = model.wv.similar_by_vector(phrase_vector, topn=10)\n",
        "    return [(similar[0], f\"{round(similar[1] * 100, 2)}%\") for similar in similar_phrases]\n",
        "\n",
        "# Find and format similar phrases for each phrase of interest\n",
        "formatted_results = {phrase: find_similar_phrases(phrase) for phrase in phrases}\n",
        "\n",
        "# Output the formatted results\n",
        "for phrase, similars in formatted_results.items():\n",
        "    print(f\"\\nPhrase: {phrase}\\nSimilar Phrases:\")\n",
        "    for similar in similars:\n",
        "        print(f\"{similar[0]} - Similarity Score: {similar[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWELJ2z6-S-Y",
        "outputId": "28c726db-f44f-46a5-af78-a4f12a89f38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phrase: computer science\n",
            "Similar Phrases:\n",
            "computer - Similarity Score: 84.16%\n",
            "science - Similarity Score: 73.13%\n",
            "undergraduate - Similarity Score: 67.75%\n",
            "mathematics - Similarity Score: 65.05%\n",
            "school - Similarity Score: 64.84%\n",
            "education - Similarity Score: 64.12%\n",
            "curriculum - Similarity Score: 63.74%\n",
            "college - Similarity Score: 63.62%\n",
            "graduate - Similarity Score: 63.46%\n",
            "teaching - Similarity Score: 62.83%\n",
            "\n",
            "Phrase: resource management\n",
            "Similar Phrases:\n",
            "resource - Similarity Score: 83.03%\n",
            "management - Similarity Score: 73.91%\n",
            "accounting - Similarity Score: 61.64%\n",
            "sla - Similarity Score: 61.44%\n",
            "managing - Similarity Score: 61.02%\n",
            "capacity - Similarity Score: 60.21%\n",
            "enterprise - Similarity Score: 59.76%\n",
            "manager - Similarity Score: 59.65%\n",
            "resources - Similarity Score: 59.45%\n",
            "policies - Similarity Score: 59.19%\n",
            "\n",
            "Phrase: natural language processing\n",
            "Similar Phrases:\n",
            "language - Similarity Score: 73.42%\n",
            "natural - Similarity Score: 66.51%\n",
            "processing - Similarity Score: 64.94%\n",
            "linguistic - Similarity Score: 58.33%\n",
            "nlp - Similarity Score: 57.55%\n",
            "markup - Similarity Score: 56.46%\n",
            "translating - Similarity Score: 54.95%\n",
            "tallis - Similarity Score: 54.8%\n",
            "scenery - Similarity Score: 53.38%\n",
            "sublanguage - Similarity Score: 53.38%\n",
            "\n",
            "Phrase: performance evaluation\n",
            "Similar Phrases:\n",
            "performance - Similarity Score: 90.58%\n",
            "evaluation - Similarity Score: 71.42%\n",
            "substantial - Similarity Score: 63.73%\n",
            "significant - Similarity Score: 61.76%\n",
            "evaluating - Similarity Score: 58.34%\n",
            "dramatic - Similarity Score: 58.16%\n",
            "biblical - Similarity Score: 54.01%\n",
            "efficiency - Similarity Score: 53.13%\n",
            "scalability - Similarity Score: 52.01%\n",
            "greatly - Similarity Score: 51.38%\n",
            "\n",
            "Phrase: data structure\n",
            "Similar Phrases:\n",
            "structure - Similarity Score: 81.88%\n",
            "data - Similarity Score: 78.85%\n",
            "structures - Similarity Score: 69.4%\n",
            "rsync - Similarity Score: 55.7%\n",
            "redaction - Similarity Score: 54.19%\n",
            "warehouses - Similarity Score: 54.01%\n",
            "secondary - Similarity Score: 53.11%\n",
            "structural - Similarity Score: 52.84%\n",
            "locality - Similarity Score: 51.68%\n",
            "phosphorylated - Similarity Score: 51.41%\n",
            "\n",
            "Phrase: artificial intelligence\n",
            "Similar Phrases:\n",
            "intelligence - Similarity Score: 93.12%\n",
            "artificial - Similarity Score: 92.22%\n",
            "ai - Similarity Score: 74.72%\n",
            "inteligencia - Similarity Score: 66.95%\n",
            "inspiration - Similarity Score: 62.8%\n",
            "ants - Similarity Score: 61.94%\n",
            "immune - Similarity Score: 60.99%\n",
            "inspired - Similarity Score: 60.98%\n",
            "ambient - Similarity Score: 60.33%\n",
            "anns - Similarity Score: 59.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "These results show that the model was succesful in identifying similar terms and succesfully captured the semantic relationships between them."
      ],
      "metadata": {
        "id": "ZdirX4kNDr5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2"
      ],
      "metadata": {
        "id": "NFdzZVq_7tEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(fashion_train_images, fashion_train_labels), (fashion_test_images, fashion_test_labels) = datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values of the train and test images\n",
        "fashion_train_images = fashion_train_images / 255.0\n",
        "fashion_test_images = fashion_test_images / 255.0\n",
        "\n",
        "# Reshape the images to include the channel dimension\n",
        "fashion_train_images = fashion_train_images.reshape((fashion_train_images.shape[0], 28, 28, 1))\n",
        "fashion_test_images = fashion_test_images.reshape((fashion_test_images.shape[0], 28, 28, 1))\n"
      ],
      "metadata": {
        "id": "Ahm4WdwEIYij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7335016-e312-43c0-a4c3-e4542632c071"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a) Constructing the Model"
      ],
      "metadata": {
        "id": "FJu8qdRD7xt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    # Convolution layer with 32 filters of size 3x3\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), strides=(1, 1)),\n",
        "    # MaxPooling layer with pool size 2x2\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # Flatten the previous layer's output to feed it into the fully connected layer\n",
        "    layers.Flatten(),\n",
        "    # Fully connected layer with 100 hidden units and relu activation function\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    # Softmax layer for classification\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "nCFqew7hov9G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(fashion_train_images, fashion_train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "#Evaluating the model\n",
        "test_loss, test_acc = model.evaluate(fashion_test_images, fashion_test_labels)\n",
        "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9f0g5FUoyXr",
        "outputId": "228563f9-e878-4239-e800-e05ce35cde87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 0.8644 - accuracy: 0.7078 - val_loss: 0.5628 - val_accuracy: 0.8043\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 0.5463 - accuracy: 0.8053 - val_loss: 0.5186 - val_accuracy: 0.8118\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 23s 30ms/step - loss: 0.4936 - accuracy: 0.8249 - val_loss: 0.4935 - val_accuracy: 0.8252\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 0.4585 - accuracy: 0.8386 - val_loss: 0.4416 - val_accuracy: 0.8445\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 0.4359 - accuracy: 0.8469 - val_loss: 0.4427 - val_accuracy: 0.8393\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 0.4167 - accuracy: 0.8543 - val_loss: 0.4079 - val_accuracy: 0.8543\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 0.4032 - accuracy: 0.8578 - val_loss: 0.4011 - val_accuracy: 0.8587\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 23s 30ms/step - loss: 0.3883 - accuracy: 0.8625 - val_loss: 0.3957 - val_accuracy: 0.8563\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 23s 30ms/step - loss: 0.3757 - accuracy: 0.8674 - val_loss: 0.3885 - val_accuracy: 0.8630\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 0.3666 - accuracy: 0.8702 - val_loss: 0.3715 - val_accuracy: 0.8690\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3879 - accuracy: 0.8622\n",
            "Test Accuracy: 0.8622000217437744, Test Loss: 0.387861430644989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy:** 0.8622\n",
        "\n",
        "\n",
        "**Test Loss:** 0.3879\n",
        "\n",
        "**Observations:**\n",
        "The model achieves a commendable accuracy of 86% and the loss saw a steady decline. This indicates that the model was learning effectively with a learning rate of 0.01"
      ],
      "metadata": {
        "id": "sXUD_tas_Cyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# b) Playing with the Learning Rates"
      ],
      "metadata": {
        "id": "zlYtM7_R7mHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-5),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(fashion_train_images, fashion_train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "#Evaluating the model\n",
        "test_loss, test_acc = model.evaluate(fashion_test_images, fashion_test_labels)\n",
        "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
      ],
      "metadata": {
        "id": "6WO_fy2_o15o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbef6ed1-e438-42fe-f17b-7b9b49fa19d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 30s 39ms/step - loss: 2.3059 - accuracy: 0.1582 - val_loss: 2.3047 - val_accuracy: 0.1619\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 31s 42ms/step - loss: 2.3020 - accuracy: 0.1660 - val_loss: 2.3008 - val_accuracy: 0.1694\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 2.2981 - accuracy: 0.1732 - val_loss: 2.2969 - val_accuracy: 0.1760\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.2943 - accuracy: 0.1805 - val_loss: 2.2931 - val_accuracy: 0.1823\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 24s 32ms/step - loss: 2.2906 - accuracy: 0.1882 - val_loss: 2.2894 - val_accuracy: 0.1903\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 24s 33ms/step - loss: 2.2870 - accuracy: 0.1958 - val_loss: 2.2857 - val_accuracy: 0.1984\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 31s 42ms/step - loss: 2.2834 - accuracy: 0.2033 - val_loss: 2.2821 - val_accuracy: 0.2058\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 28s 37ms/step - loss: 2.2798 - accuracy: 0.2109 - val_loss: 2.2785 - val_accuracy: 0.2142\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 25s 33ms/step - loss: 2.2763 - accuracy: 0.2198 - val_loss: 2.2750 - val_accuracy: 0.2225\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.2728 - accuracy: 0.2291 - val_loss: 2.2715 - val_accuracy: 0.2312\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.2713 - accuracy: 0.2317\n",
            "Test Accuracy: 0.23170000314712524, Test Loss: 2.2713232040405273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy:** 0.2317\n",
        "\n",
        "**Test Loss:** 2.2713\n",
        "\n",
        "**Observation:** The model achieves a poor accuracy of 23% with the extremely low learning rate showing that it struggled to learn. This suggests that the learning rate was too low for the model to make any changes to its weights."
      ],
      "metadata": {
        "id": "LX-o12v7_cG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(fashion_train_images, fashion_train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "#Evaluating the model\n",
        "test_loss, test_acc = model.evaluate(fashion_test_images, fashion_test_labels)\n",
        "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAxfOr-q8sVq",
        "outputId": "c0aebef0-ca09-4e0a-c6b9-3bec462b4658"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 24s 31ms/step - loss: 2.3073 - accuracy: 0.1069 - val_loss: 2.3161 - val_accuracy: 0.0957\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 22s 29ms/step - loss: 2.3061 - accuracy: 0.1011 - val_loss: 2.3057 - val_accuracy: 0.1027\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 22s 29ms/step - loss: 2.3063 - accuracy: 0.1013 - val_loss: 2.3043 - val_accuracy: 0.1013\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 22s 29ms/step - loss: 2.3062 - accuracy: 0.1004 - val_loss: 2.3046 - val_accuracy: 0.1030\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 23s 31ms/step - loss: 2.3064 - accuracy: 0.0994 - val_loss: 2.3089 - val_accuracy: 0.1013\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 22s 29ms/step - loss: 2.3060 - accuracy: 0.1002 - val_loss: 2.3067 - val_accuracy: 0.0983\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 22s 29ms/step - loss: 2.3063 - accuracy: 0.0998 - val_loss: 2.3088 - val_accuracy: 0.0995\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 22s 29ms/step - loss: 2.3063 - accuracy: 0.1013 - val_loss: 2.3087 - val_accuracy: 0.1027\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 23s 31ms/step - loss: 2.3064 - accuracy: 0.0996 - val_loss: 2.3078 - val_accuracy: 0.0957\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 23s 31ms/step - loss: 2.3062 - accuracy: 0.0984 - val_loss: 2.3034 - val_accuracy: 0.1027\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.3037 - accuracy: 0.1000\n",
            "Test Accuracy: 0.10000000149011612, Test Loss: 2.3036770820617676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy:** 0.1000\n",
        "\n",
        "**Test Loss:** 2.3037\n",
        "\n",
        "**Observation:** The model achieves a horrendous 10% accuracy with this very high learning rate, which shows that its performance was very poor. This shows that the very high learning rate caused the model to overshoot and fail to converge."
      ],
      "metadata": {
        "id": "tA5drouh_z03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c) Adding a convolution layer with 64 filters"
      ],
      "metadata": {
        "id": "ESC0fo9S92vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "Dg9vS2ni92Nj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(fashion_train_images, fashion_train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "#Evaluating the model\n",
        "test_loss, test_acc = model.evaluate(fashion_test_images, fashion_test_labels)\n",
        "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNRHmbi2CB4J",
        "outputId": "892846cb-7a33-4020-ff5f-f9e472c9d16a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 48s 62ms/step - loss: 1.1499 - accuracy: 0.5957 - val_loss: 0.7127 - val_accuracy: 0.7501\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.6865 - accuracy: 0.7421 - val_loss: 0.6055 - val_accuracy: 0.7752\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.5845 - accuracy: 0.7808 - val_loss: 0.5416 - val_accuracy: 0.8029\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.5285 - accuracy: 0.8040 - val_loss: 0.5002 - val_accuracy: 0.8194\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.4921 - accuracy: 0.8200 - val_loss: 0.4693 - val_accuracy: 0.8317\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 43s 57ms/step - loss: 0.4622 - accuracy: 0.8323 - val_loss: 0.4618 - val_accuracy: 0.8344\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 0.4403 - accuracy: 0.8404 - val_loss: 0.4452 - val_accuracy: 0.8416\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 45s 60ms/step - loss: 0.4224 - accuracy: 0.8485 - val_loss: 0.4239 - val_accuracy: 0.8510\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 0.4064 - accuracy: 0.8549 - val_loss: 0.4017 - val_accuracy: 0.8546\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 48s 63ms/step - loss: 0.3921 - accuracy: 0.8587 - val_loss: 0.3976 - val_accuracy: 0.8583\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.4151 - accuracy: 0.8551\n",
            "Test Accuracy: 0.8550999760627747, Test Loss: 0.41508257389068604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy:** 0.8551\n",
        "\n",
        "**Test Loss:** 0.4151\n",
        "\n",
        "**Observation:** The addition of the convolutional layer has had a positive imapct on the performance of the model, when compared to the base model. This shows that the convolutional layer results in better performance in image classification tasks.  "
      ],
      "metadata": {
        "id": "Rw4Ja1CcEnpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d) Adding Momentum"
      ],
      "metadata": {
        "id": "AOiOMmFt-OM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "history = model.fit(fashion_train_images, fashion_train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "#Evaluating the model\n",
        "test_loss, test_acc = model.evaluate(fashion_test_images, fashion_test_labels)\n",
        "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krmeGIE9-Hu3",
        "outputId": "b965da72-9481-45f2-f9e8-fb3836353cd4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 45s 59ms/step - loss: 0.6950 - accuracy: 0.7433 - val_loss: 0.4817 - val_accuracy: 0.8277\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.4254 - accuracy: 0.8445 - val_loss: 0.3851 - val_accuracy: 0.8622\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 0.3640 - accuracy: 0.8662 - val_loss: 0.3657 - val_accuracy: 0.8624\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 0.3272 - accuracy: 0.8802 - val_loss: 0.3276 - val_accuracy: 0.8786\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.2994 - accuracy: 0.8890 - val_loss: 0.3192 - val_accuracy: 0.8804\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 47s 62ms/step - loss: 0.2814 - accuracy: 0.8950 - val_loss: 0.3112 - val_accuracy: 0.8810\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 53s 71ms/step - loss: 0.2660 - accuracy: 0.9009 - val_loss: 0.2911 - val_accuracy: 0.8934\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 49s 65ms/step - loss: 0.2492 - accuracy: 0.9081 - val_loss: 0.2841 - val_accuracy: 0.8952\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 52s 69ms/step - loss: 0.2360 - accuracy: 0.9124 - val_loss: 0.2894 - val_accuracy: 0.8955\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 0.2221 - accuracy: 0.9171 - val_loss: 0.2736 - val_accuracy: 0.9028\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2835 - accuracy: 0.8978\n",
            "Test Accuracy: 0.8978000283241272, Test Loss: 0.2835187017917633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Accuracy:** 0.8978\n",
        "\n",
        "**Test Loss:** 0.2835\n",
        "\n",
        "**Observation:** The addition of momentum has further enhanced the performance, resulting in the highest accuracy of all the tests, alongside decreased loss. Furthemrore, the model converges faster and generalizes better, which can be seen by the higher accuracy achieved in lesser epochs, and the higher test accuracy and lower test loss.\n",
        "\n",
        "To summarize, the addition of momentum leads to the model learning better and uncovering more underlying patterns resulting in the best performance out of all the tests conducted."
      ],
      "metadata": {
        "id": "YW1GcArlCLMl"
      }
    }
  ]
}